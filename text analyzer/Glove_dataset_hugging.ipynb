{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_dataset_hugging",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuLkycmR3gGsVJpbWPy15/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JA-Bar/multimodal-sentiment-analysis/blob/kevin/Glove_dataset_hugging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPduRyT8Tre7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhZta4aqUiqd"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "   'emotion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm0_0Fja7ykF"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "!pip install -q sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujJ3ZU8z-sCV"
      },
      "source": [
        "!pip install tensorflow-text\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-fTUThaCEPY"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM,SimpleRNN,GRU,RNN\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from official.nlp import optimization \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prsponiIS0YK"
      },
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHArRknaFDl3"
      },
      "source": [
        "def adding_new_data(dataframe_input):\n",
        "  place=0\n",
        "  for sent in dataframe_input['sentiment']:\n",
        "    if sent==0:\n",
        "      dataframe_input['sadness'][place]=1\n",
        "    elif sent==1:\n",
        "      dataframe_input['joy'][place]=1\n",
        "    elif sent==2:\n",
        "      dataframe_input['love'][place]=1\n",
        "    elif sent==3:\n",
        "      dataframe_input['anger'][place]=1\n",
        "    elif sent==4:\n",
        "      dataframe_input['fear'][place]=1\n",
        "    elif sent==5:\n",
        "      dataframe_input['unsolve'][place]=1\n",
        "    place += 1\n",
        "  dataframe_input['sadness']=dataframe_input['sadness'].replace(np.nan, 0)\n",
        "  dataframe_input['joy']=dataframe_input['joy'].replace(np.nan, 0)\n",
        "  dataframe_input['love']=dataframe_input['love'].replace(np.nan, 0)\n",
        "  dataframe_input['anger']=dataframe_input['anger'].replace(np.nan, 0)\n",
        "  dataframe_input['fear']=dataframe_input['fear'].replace(np.nan, 0)\n",
        "  dataframe_input['unsolve']=dataframe_input['unsolve'].replace(np.nan,0)\n",
        "  return dataframe_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04sf4etFDoSH"
      },
      "source": [
        "def data (dataset_raw,x):\n",
        "  data=dataset_raw[x]\n",
        "  labels=data['label']\n",
        "  text=data['text']\n",
        "  text=np.array([text])\n",
        "  labels=np.array([labels])\n",
        "  data4=pd.DataFrame([text[0],labels[0]])\n",
        "  data4=data4.T\n",
        "  data4.columns=['text','sentiment']\n",
        "  data4[\"sadness\"] = np.nan\n",
        "  data4[\"joy\"] = np.nan\n",
        "  data4[\"love\"] = np.nan\n",
        "  data4[\"anger\"] = np.nan\n",
        "  data4[\"fear\"] = np.nan\n",
        "  data4[\"unsolve\"] = np.nan\n",
        "  data4=adding_new_data(data4)\n",
        "  return data4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2jZ4EvFDg-7"
      },
      "source": [
        "train=data (dataset,'train')\n",
        "test=data (dataset,'test')\n",
        "validation=data (dataset,'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWPBWkjaFNjM"
      },
      "source": [
        "train.to_csv('train.csv')\n",
        "test.to_csv('test.csv')\n",
        "validation.to_csv('validation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tPr-sL9K12c"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyj-FRdTSwpf"
      },
      "source": [
        "print(train.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4IQlbP1SNp6"
      },
      "source": [
        "concat_data=[train,test,validation]\n",
        "train = pd.concat(concat_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTXRvWiiOgw_"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NQ1JJKkStV3"
      },
      "source": [
        "print(train.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXETLy8wCMV-"
      },
      "source": [
        "place=168\n",
        "filter = train[\"text\"] != \"\"\n",
        "train = train[filter]\n",
        "train = train.dropna()\n",
        "print('text: ' + train[\"text\"][place])\n",
        "print(\"label:\" + str(train[\"sentiment\"][place]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyeAOhy0CUJA"
      },
      "source": [
        "#train_labels = train[[\"sadness\", \"joy\", \"love\",'anger','fear','unsolve']]\n",
        "train_labels = train[[\"sadness\", \"joy\"]]\n",
        "#train_labels = train[[\"joy\",\"sadness\"]]\n",
        "classes=train_labels.shape\n",
        "classes=classes[1]\n",
        "print(classes)\n",
        "train_labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiqrSE93CXFu"
      },
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 10\n",
        "fig_size[1] = 8\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "train_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-WLnbOCZmX"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xecj732mCb3m"
      },
      "source": [
        "X = []\n",
        "sentences = list(train[\"text\"])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "y = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Z2leyZY1ND"
      },
      "source": [
        "X[1:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJSU0dKaCdHH"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24jVF6n90mLx"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QsO-tuCfO3"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkMsCXi9UBU"
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCfrvESv9VRz"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = None\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz6GXBWxBMVP"
      },
      "source": [
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx4eLO6jBMX-"
      },
      "source": [
        "print(X_train[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8wuE5-VCnTR"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/gdrive/MyDrive/Clasificador/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "#print(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_h1oWsw8Bs0"
      },
      "source": [
        "deep_inputs = Input(shape=(None,))\n",
        "print(deep_inputs)\n",
        "net = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False,mask_zero=True,name = 'embeddings')(deep_inputs)\n",
        "print(net)\n",
        "net = LSTM(200, return_sequences=True,name='lstm_layer1')(net)\n",
        "net = SimpleRNN(200, return_sequences=True,name='RNN')(net)\n",
        "net = GRU(100, return_sequences=True,name='GRU')(net)\n",
        "net = LSTM(150,name='lstm_layer2')(net)\n",
        "#net = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation=None, kernel_initializer='he_uniform')(net)\n",
        "#net = tf.keras.layers.MaxPooling1D(3)(net)\n",
        "#net = tf.keras.layers.GlobalMaxPool1D()(net)\n",
        "#net = tf.keras.layers.BatchNormalization()(net)\n",
        "#net=tf.keras.layers.Dropout(0.2)(net)\n",
        "#net = Dense(25,activation=None)(net)\n",
        "#net = Dense(12,activation=None)(net)\n",
        "#net = Dense(6,activation=None)(net)\n",
        "net = Dense(classes,activation='sigmoid')(net)\n",
        "model2= Model(inputs=deep_inputs, outputs=net)\n",
        "opt = tf.optimizers.Adam(learning_rate=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mC1k14CqwZ"
      },
      "source": [
        "history = model2.fit(X_train, y_train, batch_size=16, epochs=50, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRmJslaIC6ku"
      },
      "source": [
        "score = model2.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBRy2FYfKowK"
      },
      "source": [
        "def convert2embeding (word):\n",
        "  X = []\n",
        "  sentences = list(word)\n",
        "  #print(sentences)\n",
        "  for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "  #print(X)\n",
        "  token_word=tokenizer.texts_to_sequences(X[0])\n",
        "  #print(token_word)\n",
        "  final=[]\n",
        "  for tok in token_word:\n",
        "    try:\n",
        "      final.append(tok[0])\n",
        "    except:\n",
        "      final.append(0)\n",
        "  #print(final)\n",
        "  maxlen = 50\n",
        "  final_word = pad_sequences([final], padding='post', maxlen=maxlen)\n",
        "  #print(final_word)\n",
        "  return final_word\n",
        "word2='Beatiful world baby'\n",
        "x=convert2embeding ([word2])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEIXC7SiAtFB"
      },
      "source": [
        "path='/content/gdrive/MyDrive/Clasificador/glove_dataset_hugging_face.h5'\n",
        "model2.save(path)\n",
        "new_model = keras.models.load_model(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVJ_Zt-nRlQq"
      },
      "source": [
        "[\"sadness\", \"joy\", \"love\",'anger','fear','unsolve']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y6FkI5IMx3a"
      },
      "source": [
        "#Many\n",
        "y='beatiful world baby'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,' ',a)\n",
        "y='hate you'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='very bored'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i was so happy back then'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i am so happy'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='is so scary'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i hate my life'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i love you'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='letÂ´s party all night!!!!'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='lets have some fun baby'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i want to kill myself'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='fucking bitch'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y= 'i am so sad'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y= 'i used to be happy'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PSynsZ2fBnj"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
