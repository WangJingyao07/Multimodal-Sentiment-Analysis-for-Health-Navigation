{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fasttext_dataset_IEMOCAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkE6cbKnhyXTH8umiwCCN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JA-Bar/multimodal-sentiment-analysis/blob/kevin/Fasttext_dataset_IEMOCAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPduRyT8Tre7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhZta4aqUiqd"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "   'emotion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm0_0Fja7ykF"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "!pip install -q sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujJ3ZU8z-sCV"
      },
      "source": [
        "!pip install tensorflow-text\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-fTUThaCEPY"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM,SimpleRNN,GRU,RNN\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from official.nlp import optimization \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prsponiIS0YK"
      },
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNyQtXKwVZpZ"
      },
      "source": [
        "dataset=pd.read_csv('/content/gdrive/MyDrive/Clasificador/text_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHArRknaFDl3"
      },
      "source": [
        "def adding_new_data(dataframe_input):\n",
        "  place=0\n",
        "  for sent in dataframe_input['sentiment']:\n",
        "    if sent==0:\n",
        "      dataframe_input['anger'][place]=1\n",
        "    elif sent==1:\n",
        "      dataframe_input['happy'][place]=1\n",
        "    elif sent==2:\n",
        "      dataframe_input['exc'][place]=1\n",
        "    elif sent==3:\n",
        "      dataframe_input['sad'][place]=1\n",
        "    elif sent==4:\n",
        "      dataframe_input['frustraded'][place]=1\n",
        "    elif sent==5:\n",
        "      dataframe_input['fear'][place]=1\n",
        "    elif sent==6:\n",
        "      dataframe_input['sur'][place]=1\n",
        "    elif sent==7:\n",
        "      dataframe_input['neu'][place]=1\n",
        "    elif sent==8:\n",
        "      dataframe_input['xxx/outh'][place]=1\n",
        "    place += 1\n",
        "  dataframe_input['anger']=dataframe_input['anger'].replace(np.nan, 0)\n",
        "  dataframe_input['happy']=dataframe_input['happy'].replace(np.nan, 0)\n",
        "  dataframe_input['exc']=dataframe_input['exc'].replace(np.nan, 0)\n",
        "  dataframe_input['sad']=dataframe_input['sad'].replace(np.nan, 0)\n",
        "  dataframe_input['frustraded']=dataframe_input['frustraded'].replace(np.nan, 0)\n",
        "  dataframe_input['fear']=dataframe_input['fear'].replace(np.nan,0)\n",
        "  dataframe_input['sur']=dataframe_input['sur'].replace(np.nan,0)\n",
        "  dataframe_input['neu']=dataframe_input['neu'].replace(np.nan,0)\n",
        "  dataframe_input['xxx/outh']=dataframe_input['xxx/outh'].replace(np.nan,0)\n",
        "  return dataframe_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04sf4etFDoSH"
      },
      "source": [
        "def data (dataset_raw,x):\n",
        "  data=dataset_raw\n",
        "  labels=data['label']\n",
        "  text=data['transcription']\n",
        "  text=np.array([text])\n",
        "  labels=np.array([labels])\n",
        "  data4=pd.DataFrame([text[0],labels[0]])\n",
        "  data4=data4.T\n",
        "  data4.columns=['text','sentiment']\n",
        "  data4[\"anger\"] = np.nan\n",
        "  data4[\"happy\"] = np.nan\n",
        "  data4[\"exc\"] = np.nan\n",
        "  data4[\"sad\"] = np.nan\n",
        "  data4[\"frustraded\"] = np.nan\n",
        "  data4[\"fear\"] = np.nan\n",
        "  data4[\"sur\"] = np.nan\n",
        "  data4[\"neu\"] = np.nan\n",
        "  data4[\"xxx/outh\"] = np.nan\n",
        "  data4=adding_new_data(data4)\n",
        "  return data4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2jZ4EvFDg-7"
      },
      "source": [
        "train=data (dataset,'train')\n",
        "test=data (dataset,'test')\n",
        "validation=data (dataset,'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWPBWkjaFNjM"
      },
      "source": [
        "train.to_csv('train.csv')\n",
        "test.to_csv('test.csv')\n",
        "validation.to_csv('validation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tPr-sL9K12c"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyj-FRdTSwpf"
      },
      "source": [
        "print(train.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4IQlbP1SNp6"
      },
      "source": [
        "concat_data=[train,test,validation]\n",
        "train = pd.concat(concat_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTXRvWiiOgw_"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NQ1JJKkStV3"
      },
      "source": [
        "print(train.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXETLy8wCMV-"
      },
      "source": [
        "place=168\n",
        "filter = train[\"text\"] != \"\"\n",
        "train = train[filter]\n",
        "train = train.dropna()\n",
        "print('text: ' + train[\"text\"][place])\n",
        "print(\"label:\" + str(train[\"sentiment\"][place]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyeAOhy0CUJA"
      },
      "source": [
        "#train_labels = train[['anger','happy','exc','sad','frustraded','sur','neu','xxx/outh']]\n",
        "#train_labels = train[['anger','sad','frustraded','neu']]\n",
        "#train_labels = train[['anger','frustraded','neu']]\n",
        "train_labels = train[['frustraded','neu']]\n",
        "classes=train_labels.shape\n",
        "classes=classes[1]\n",
        "print(classes)\n",
        "train_labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiqrSE93CXFu"
      },
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 10\n",
        "fig_size[1] = 8\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "train_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-WLnbOCZmX"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xecj732mCb3m"
      },
      "source": [
        "X = []\n",
        "sentences = list(train[\"text\"])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "y = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Z2leyZY1ND"
      },
      "source": [
        "X[1:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJSU0dKaCdHH"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24jVF6n90mLx"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QsO-tuCfO3"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkMsCXi9UBU"
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCfrvESv9VRz"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = None\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz6GXBWxBMVP"
      },
      "source": [
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx4eLO6jBMX-"
      },
      "source": [
        "print(X_train[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8wuE5-VCnTR"
      },
      "source": [
        "embedding_dim_fasttext = 300\n",
        "embeddings_index_fasttext = {}\n",
        "word_index=tokenizer.word_index\n",
        "f = open('/content/gdrive/MyDrive/Clasificador/wiki-news-300d-1M.vec', encoding='utf8')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()\n",
        "print(len(word_index) + 1)\n",
        "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index_fasttext.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix_fasttext[i] = embedding_vector\n",
        "#print(embedding_matrix_fasttext)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_h1oWsw8Bs0"
      },
      "source": [
        "deep_inputs = Input(shape=(None,))\n",
        "net = Embedding(vocab_size, 300, weights=[embedding_matrix_fasttext], trainable=False,mask_zero=True,name = 'embeddings')(deep_inputs)\n",
        "\n",
        "#activations\n",
        "#act='relu'\n",
        "act='sigmoid'\n",
        "#act=activation=tf.keras.activations.softmax\n",
        "\n",
        "#Layer 1\n",
        "net = LSTM(150,name='lstm_layer_1',return_sequences=True)(net)\n",
        "net = SimpleRNN(150, return_sequences=True,name='RNN_1')(net)\n",
        "net = GRU(150, return_sequences=True,name='GRU_1')(net)\n",
        "\n",
        "#Layer 2\n",
        "#net = LSTM(200,name='lstm_layer_2',return_sequences=True)(net)\n",
        "#net = (SimpleRNN(200, return_sequences=True,name='RNN_2'))(net)\n",
        "#net = GRU(200, return_sequences=True,name='GRU_2')(net)\n",
        "\n",
        "#Layer 3\n",
        "net = LSTM(200,name='lstm_layer_3')(net)\n",
        "\n",
        "#net = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation=None, kernel_initializer='he_uniform')(net)\n",
        "#net = tf.keras.layers.MaxPooling1D(3)(net)\n",
        "#net = tf.keras.layers.GlobalMaxPool1D()(net)\n",
        "#net = tf.keras.layers.BatchNormalization()(net)\n",
        "\n",
        "net=tf.keras.layers.Dropout(0.3)(net)\n",
        "\n",
        "#net = Dense(25,activation=None)(net)\n",
        "#net = Dense(12,activation=None)(net)\n",
        "#net = Dense(6,activation=None)(net)\n",
        "\n",
        "net = Dense(classes,activation=act)(net)\n",
        "model2= Model(inputs=deep_inputs, outputs=net)\n",
        "\n",
        "#optimizers\n",
        "#opt = tf.optimizers.Adam(learning_rate=0.01)\n",
        "#opt='rmsprop'\n",
        "#opt=tf.keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9,momentum=0.0,epsilon=1e-07,centered=False,name=\"RMSprop\")\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
        "opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "#loss\n",
        "l='binary_crossentropy'\n",
        "#l='categorical_crossentropy'\n",
        "\n",
        "model2.compile(loss=l, optimizer=opt, metrics=['acc'])\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e8mC1k14CqwZ"
      },
      "source": [
        "history = model2.fit(X_train, y_train, batch_size=24, epochs=25, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dRmJslaIC6ku"
      },
      "source": [
        "score = model2.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lBRy2FYfKowK"
      },
      "source": [
        "def convert2embeding (word):\n",
        "  X = []\n",
        "  sentences = list(word)\n",
        "  #print(sentences)\n",
        "  for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "  #print(X)\n",
        "  token_word=tokenizer.texts_to_sequences(X[0])\n",
        "  #print(token_word)\n",
        "  final=[]\n",
        "  for tok in token_word:\n",
        "    try:\n",
        "      final.append(tok[0])\n",
        "    except:\n",
        "      final.append(0)\n",
        "  #print(final)\n",
        "  maxlen = None\n",
        "  final_word = pad_sequences([final], padding='post', maxlen=maxlen)\n",
        "  #print(final_word)\n",
        "  return final_word\n",
        "word2='Beatiful world baby'\n",
        "x=convert2embeding ([word2])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OEIXC7SiAtFB"
      },
      "source": [
        "path='/content/gdrive/MyDrive/Clasificador/fasttext_dataset_IEMOCAP.h5'\n",
        "model2.save(path)\n",
        "new_model = keras.models.load_model(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pVJ_Zt-nRlQq"
      },
      "source": [
        "[\"sadness\", \"joy\", \"love\",'anger','fear','unsolve']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "34vHjIIIWhva"
      },
      "source": [
        "model=model2\n",
        "model2=new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6xwn73bn6a-b"
      },
      "source": [
        "#Many\n",
        "y='beatiful world baby'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,' ',a)\n",
        "y='hate you'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='very bored'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i was so happy back then'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i am so happy'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='is so scary'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i hate my life'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i love you'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='let´s party all night!!!!'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='lets have some fun baby'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i want to kill myself'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='fucking bitch'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y= 'i am so sad'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y= 'i used to be happy'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PSynsZ2fBnj"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
